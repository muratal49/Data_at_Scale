{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "080d422b-bd5d-4ae2-9861-e3daaedaafe3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LIBRARIES"
    }
   },
   "outputs": [],
   "source": [
    "# ENTER YOUR CODE HERE\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import threading\n",
    "\n",
    "#Pyspark.SQL\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.streaming import StreamingQueryListener\n",
    "\n",
    "\n",
    "# Delta and MLflow\n",
    "from delta.tables import DeltaTable\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "\n",
    "# Utilities\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "#Hugging Face:\n",
    "from transformers import pipeline\n",
    "\n",
    "# ML-Flow Section \n",
    "\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, classification_report)\n",
    "\n",
    "#Additional Setup for Date-Time processing:\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d977044-8c9f-49ae-af46-e4ecd7da3a04",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PATH FOLDERS/MODEL"
    }
   },
   "outputs": [],
   "source": [
    "# Specify the raw tweet path\n",
    "TWEET_SOURCE_PATH = f\"dbfs:/FileStore/tables/raw_tweets/\"\n",
    "\n",
    "\n",
    "USER_NAME = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get().split('@')[0]\n",
    "USER_DIR = f'/tmp/{USER_NAME}/'\n",
    "TEST_PATH=USER_DIR + \"temp_files/\"\n",
    "BRONZE_CHECKPOINT = USER_DIR + 'bronze.checkpoint'\n",
    "BRONZE_DELTA = USER_DIR + 'bronze.delta'\n",
    "\n",
    "SILVER_CHECKPOINT = USER_DIR + 'silver.checkpoint'\n",
    "SILVER_DELTA = USER_DIR + 'silver.delta'\n",
    "\n",
    "GOLD_CHECKPOINT = USER_DIR + 'gold.checkpoint'\n",
    "GOLD_DELTA = USER_DIR + 'gold.delta'\n",
    "\n",
    "MODEL_NAME = \"MuratAL_HF_Sentiment\"\n",
    "#\"HF_TWEET_SENTIMENT\" #USER_NAME + \"_Model\"\n",
    "\n",
    "# https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis\n",
    "HF_MODEL_NAME = \"finiteautomata/bertweet-base-sentiment-analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb9e3148-916a-455f-8dd1-e0dde07cdc35",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "GIVEN FUNCTIONS"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to optimize Delta table if it exists\n",
    "def optimize_table(path):\n",
    "    if os.path.exists(path):\n",
    "        DeltaTable.forPath(spark, path).optimize().executeCompaction()\n",
    "        print(f\"Optimized Delta Table at {path}\")\n",
    "    else:\n",
    "        print(f\"Delta Table at {path} does not exist.\")\n",
    "\n",
    "# This routine requires the paths defined in the includes notebook\n",
    "# and it clears data from the previous run.\n",
    "def clear_previous_run() -> bool:\n",
    "    # delete previous run \n",
    "    dbutils.fs.rm(BRONZE_CHECKPOINT, True)\n",
    "    dbutils.fs.rm(BRONZE_DELTA, True)\n",
    "    dbutils.fs.rm(SILVER_CHECKPOINT, True)\n",
    "    dbutils.fs.rm(SILVER_DELTA, True)\n",
    "    dbutils.fs.rm(GOLD_CHECKPOINT, True)\n",
    "    dbutils.fs.rm(GOLD_DELTA, True)\n",
    "    return True\n",
    "\n",
    "def stop_all_streams() -> bool:\n",
    "    stopped = False\n",
    "    for stream in spark.streams.active:\n",
    "        stopped = True\n",
    "        stream.stop()\n",
    "    return stopped\n",
    "\n",
    "\n",
    "def stop_named_stream(spark: SparkSession, namedStream: str) -> bool:\n",
    "    stopped = False\n",
    "    for stream in spark.streams.active:\n",
    "        if stream.name == namedStream:\n",
    "            stopped = True \n",
    "            stream.stop()\n",
    "    return stopped\n",
    "\n",
    "def wait_stream_start(spark: SparkSession, namedStream: str) -> bool:\n",
    "    started = False\n",
    "    count = 0\n",
    "    if started == False and count <= 3:\n",
    "        for stream in spark.streams.active:\n",
    "            if stream.name == namedStream:\n",
    "                started = True\n",
    "        count += 1\n",
    "        time.sleep(10)\n",
    "    return started    \n",
    "\n",
    "# Function to wait for the Delta table to be ready\n",
    "def wait_for_delta_table(path, timeout=30, check_interval=2):\n",
    "    \"\"\"\n",
    "    Waits for a Delta table to be available before proceeding.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the Delta table.\n",
    "        timeout (int): Maximum wait time in seconds.\n",
    "        check_interval (int): Time interval to check for table availability.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the table is ready, False otherwise.\n",
    "    \"\"\"\n",
    "    elapsed_time = 0\n",
    "    while elapsed_time < timeout:\n",
    "        try:\n",
    "            if spark.read.format(\"delta\").load(path).count() > 0:\n",
    "                return True\n",
    "        except:\n",
    "            pass\n",
    "        time.sleep(check_interval)\n",
    "        elapsed_time += check_interval\n",
    "    return False\n",
    "\n",
    "# Function to retrieve streaming statistics\n",
    "def get_streaming_stats():\n",
    "    \"\"\"\n",
    "    Retrieves streaming statistics such as elapsed time, input row count, and processing time.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe containing streaming statistics for active queries.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    start_time = None  # Track when the job started\n",
    "\n",
    "    for q in spark.streams.active:\n",
    "        progress = q.recentProgress\n",
    "        if progress:\n",
    "            for p in progress:\n",
    "                timestamp = datetime.strptime(p[\"timestamp\"], \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "\n",
    "                # Set the start time on the first iteration\n",
    "                if start_time is None:\n",
    "                    start_time = timestamp\n",
    "\n",
    "                elapsed_time = (timestamp - start_time).total_seconds()  # Convert to seconds\n",
    "\n",
    "                # Check if 'addBatch' exists in 'durationMs' before accessing it\n",
    "                processing_time = p[\"durationMs\"].get(\"addBatch\", None) if \"durationMs\" in p else None\n",
    "\n",
    "                data.append({\n",
    "                    \"query\": q.name,\n",
    "                    \"elapsed_time\": elapsed_time,  # Time in seconds since job start\n",
    "                    \"input_rows\": p.get(\"numInputRows\", 0),  # Default to 0 if missing\n",
    "                    \"processing_time\": processing_time,  # Could be None if not available\n",
    "                    \"memory_used\": p.get(\"aggregatedStateOperators\", [{}])[0].get(\"stateMemory\", 0) if p.get(\"aggregatedStateOperators\") else 0\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "496c6f18-6b71-49cd-857d-661119633169",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "TESTING FUNCTIONS:"
    }
   },
   "outputs": [],
   "source": [
    "#1) Feeding function for the Test folder:\n",
    "def copy_new_test_files(num_files=50, verbose=True):\n",
    "    \"\"\"\n",
    "    Copy the most recent `num_files` from TWEET_SOURCE_PATH to TEST_PATH.\n",
    "    Files are renamed using timestamp + counter to simulate streaming ingestion.\n",
    "    Safe for use with S3/cloudFiles streaming ingestion.\n",
    "    \"\"\"\n",
    "  \n",
    "    # Ensure test directory exists (don't delete it in streaming context)\n",
    "    dbutils.fs.mkdirs(TEST_PATH)\n",
    "\n",
    "    try:\n",
    "        # Load and convert file listing\n",
    "        files = dbutils.fs.ls(TWEET_SOURCE_PATH)\n",
    "        files_df = spark.createDataFrame(files)\n",
    "    except Exception as e:\n",
    "        print(\"❌ Error listing source path files:\", str(e))\n",
    "        return\n",
    "\n",
    "    # Filter and select most recent JSON files\n",
    "    top_files = (\n",
    "        files_df\n",
    "        .filter(col(\"name\").endswith(\".json\"))\n",
    "        .orderBy(\"modificationTime\", ascending=False)\n",
    "        .limit(num_files)\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    if not top_files:\n",
    "        print(\"⚠️ No .json files found in source directory.\")\n",
    "        return\n",
    "\n",
    "    # Timestamp once, then increment counter for filenames\n",
    "    base_ts = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    for i, f in enumerate(top_files):\n",
    "        new_name = f\"{base_ts}_{i:04d}.json\"  # e.g., 20250503_0003.json\n",
    "        target_path = TEST_PATH + new_name\n",
    "        dbutils.fs.cp(f.path, target_path)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"✅ Copied {len(top_files)} test files to: {TEST_PATH}\")\n",
    "\n",
    "\n",
    "#2) Keep feeding:\n",
    "# import threading\n",
    "\n",
    "def continuously_copy_test_files(interval_sec=10, total_rounds=5, num_files=20):\n",
    "    for _ in range(total_rounds):\n",
    "        copy_new_test_files(num_files=num_files)\n",
    "        print(f\"📁 Injected {num_files} files.\")\n",
    "        time.sleep(interval_sec)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d3b85c2-6d1e-4ed1-a02d-f2edd5da8b0e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "UDF's  for HF MODEL"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1. Load the model ONCE \n",
    "# --------------------------------------------------\n",
    "   #Sentiment Model \n",
    "# hf_sentiment = pipeline(\"sentiment-analysis\",\n",
    "#                          model=HF_MODEL_NAME,\n",
    "#                          return_all_scores=True)\n",
    "\n",
    "def set_hf_sentiment_model_batch_size(batch_size: int):\n",
    "    \"\"\"\n",
    "    Factory to create a pandas UDF for sentiment analysis with given batch_size.\n",
    "    Returns a pandas UDF named `hf_sentiment`.\n",
    "    \"\"\"\n",
    "    # Initialize the HF sentiment pipeline with the desired batch_size\n",
    "    hf_sentiment = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=HF_MODEL_NAME,\n",
    "        return_all_scores=True,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    return hf_sentiment\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2. UDF – NEUTRAL SCORES to POS-NEG \n",
    "@pandas_udf(\"struct<label:string, score:double, binary:int>\")\n",
    "def get_sentiment_udf(texts: pd.Series) -> pd.DataFrame:\n",
    "    all_batches = hf_sentiment(texts.fillna(\"\").tolist())  # List[List[dict]]\n",
    "    \n",
    "    labels, scores, binaries = [], [], []\n",
    "\n",
    "    for score_list in all_batches:\n",
    "        # Sort descending by score\n",
    "        sorted_items = sorted(score_list, key=lambda x: -x[\"score\"])\n",
    "        top = sorted_items[0]\n",
    "        label = top[\"label\"]\n",
    "        score = top[\"score\"]\n",
    "\n",
    "        # Binary logic\n",
    "        if label == \"POS\":\n",
    "            b = 1\n",
    "        elif label == \"NEG\":\n",
    "            b = 0\n",
    "        elif label == \"NEU\":\n",
    "            # NEU is top → check second best\n",
    "            second = sorted_items[1][\"label\"]\n",
    "            if second == \"POS\":\n",
    "                b = 1\n",
    "            else:  # assume second is NEG or fallback\n",
    "                b = 0\n",
    "        else:\n",
    "            # Unexpected label (safety fallback)\n",
    "            b = 0\n",
    "\n",
    "        labels.append(label)\n",
    "        scores.append(score)\n",
    "        binaries.append(b)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"label\": labels,\n",
    "        \"score\": scores,\n",
    "        \"binary\": binaries\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "#TO extract multiple mentions:\n",
    "@udf(ArrayType(StringType()))\n",
    "def extract_mentions(text):\n",
    "    if text:\n",
    "        return re.findall(r\"@\\w+\", text)\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7efc6c40-1394-489c-98b6-e85bc92330bd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "3) BRONZE STREAM"
    }
   },
   "outputs": [],
   "source": [
    "#3) Stop Streams and Start BRONZE: \n",
    "\n",
    "def bronze_restart(source_path=TEST_PATH, delay_between_starts=5):\n",
    "    \n",
    "    print(\"🔄 Stopping all active streams...\")\n",
    "\n",
    "    for stream in spark.streams.active:\n",
    "        stream.stop()\n",
    "    print(\"✅ All active streams stopped.\")\n",
    "\n",
    "    dbutils.fs.rm(BRONZE_CHECKPOINT, recurse=True)\n",
    "    dbutils.fs.rm(SILVER_CHECKPOINT, recurse=True)\n",
    "    dbutils.fs.rm(GOLD_CHECKPOINT, recurse=True)\n",
    "    print(\"🧹 All checkpoint directories cleared.\")\n",
    "\n",
    "    bronze_schema = StructType([\n",
    "        StructField(\"date\", StringType(), True),\n",
    "        StructField(\"sentiment\", StringType(), True),\n",
    "        StructField(\"text\", StringType(), True),\n",
    "        StructField(\"user\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "    bronze_stream = (\n",
    "        spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .schema(bronze_schema)\n",
    "            .option(\"cloudFiles.format\", \"json\")\n",
    "            .load(source_path)\n",
    "            .withColumn(\"source_file\", input_file_name())\n",
    "            .withColumn(\"processing_time\", current_timestamp())\n",
    "    )\n",
    "\n",
    "    bronze_query=(\n",
    "        bronze_stream.writeStream\n",
    "            .format(\"delta\")\n",
    "            .outputMode(\"append\")\n",
    "            .option(\"checkpointLocation\", BRONZE_CHECKPOINT)\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .queryName(\"bronze_stream\")\n",
    "            .start(BRONZE_DELTA)\n",
    "    )\n",
    "    print(\"🚀 Bronze stream restarted.\")\n",
    "    bronze_stream.printSchema()\n",
    "    return bronze_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "908a95fb-9fdb-4596-b504-7447dea3ee6c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "4) SILVER STREAM"
    }
   },
   "outputs": [],
   "source": [
    "# SILVER STREAM:\n",
    "def silver_restart(delay_between_starts=5):\n",
    "\n",
    "    if not wait_for_delta_table(BRONZE_DELTA, timeout=60):\n",
    "        print(\"⚠️ Bronze Delta not ready. Skipping Silver stream.\")\n",
    "        return\n",
    "\n",
    "    time.sleep(delay_between_starts)\n",
    "\n",
    "    silver_stream_df = (\n",
    "        spark.readStream\n",
    "            .format(\"delta\")\n",
    "            .load(BRONZE_DELTA)\n",
    "            .withColumn(\"timestamp\", to_timestamp(regexp_replace(col(\"date\"), \" [A-Z]{3} \", \" \"), \"EEE MMM dd HH:mm:ss yyyy\"))\n",
    "            .withColumn(\"mention\", explode(extract_mentions(col(\"text\"))))\n",
    "            .withColumn(\"cleaned_text\", regexp_replace(col(\"text\"), \"@\\\\w+\", \"\"))\n",
    "            .filter(col(\"cleaned_text\").isNotNull() & (length(trim(col(\"cleaned_text\"))) > 0))\n",
    "            .select(\"timestamp\", \"mention\", \"cleaned_text\", \"sentiment\")\n",
    "    )\n",
    "\n",
    "    silver_query=(\n",
    "        silver_stream_df.writeStream\n",
    "            .format(\"delta\")\n",
    "            .outputMode(\"append\")\n",
    "            .option(\"checkpointLocation\", SILVER_CHECKPOINT)\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .queryName(\"silver_stream\")\n",
    "            .start(SILVER_DELTA)\n",
    "    )\n",
    "    print(\"🚀 Silver stream restarted.\")\n",
    "    silver_stream_df.printSchema()\n",
    "    return silver_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d072bc8a-04bd-4fbb-be1f-1899349391fa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "5) GOLD STREAM"
    }
   },
   "outputs": [],
   "source": [
    " \n",
    "def gold_restart(delay_between_starts=5):\n",
    "\n",
    "    if not wait_for_delta_table(SILVER_DELTA, timeout=60):\n",
    "        print(\"⚠️ Silver Delta not ready. Skipping Gold stream.\")\n",
    "        return\n",
    "\n",
    "    time.sleep(delay_between_starts)\n",
    "\n",
    "    gold_df = (\n",
    "        spark.readStream\n",
    "            .format(\"delta\")\n",
    "            .load(SILVER_DELTA)\n",
    "            .filter(col(\"cleaned_text\").isNotNull() & (length(trim(col(\"cleaned_text\"))) > 0))\n",
    "            .withColumn(\"sentiment_result\", get_sentiment_udf(col(\"cleaned_text\")))\n",
    "         .withColumn(\"predicted_sentiment\",      col(\"sentiment_result.label\"))\n",
    "         .withColumn(\"predicted_score\",          col(\"sentiment_result.score\"))\n",
    "         .withColumn(\"predicted_sentiment_id\",   col(\"sentiment_result.binary\"))\n",
    "         # true label → ID\n",
    "         .withColumn(\n",
    "            \"sentiment_id\",\n",
    "             expr(\"CASE WHEN lower(sentiment) = 'positive' THEN 1 ELSE 0 END\")\n",
    "         )\n",
    "         .drop(\"sentiment_result\")\n",
    ")\n",
    "\n",
    "    gold_query=(\n",
    "        gold_df.writeStream\n",
    "            .format(\"delta\")\n",
    "            .outputMode(\"append\")\n",
    "            .option(\"checkpointLocation\", GOLD_CHECKPOINT)\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .queryName(\"gold_stream\")\n",
    "            # .trigger(once=True)\n",
    "            .trigger(processingTime='10 seconds')\n",
    "            .start(GOLD_DELTA)\n",
    "    )\n",
    "    \n",
    "    print(\"🚀 Gold stream restarted.\")\n",
    "    gold_df.printSchema()\n",
    "    return gold_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "013eb82e-b3cc-4812-b74b-96d9f4d9e4ea",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "6.a) MONITORING:"
    }
   },
   "outputs": [],
   "source": [
    "def monitor_streams(interval_sec=10):\n",
    "    \"\"\"\n",
    "    Poll Spark streams one time, sleeping interval_sec beforehand,\n",
    "    and return a list of dicts with keys:\n",
    "      - timestamp (str)\n",
    "      - query     (stream.name)\n",
    "      - input_rows\n",
    "      - processing_time (ms)\n",
    "    \"\"\"\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "\n",
    "    # wait for the interval (so you get fresh data)\n",
    "    time.sleep(interval_sec)\n",
    "\n",
    "    metrics = []\n",
    "    for stream in spark.streams.active:\n",
    "        prog = stream.lastProgress\n",
    "        if prog:\n",
    "            metrics.append({\n",
    "                \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"query\":      stream.name or \"<unnamed>\",\n",
    "                \"input_rows\": prog.get(\"numInputRows\", 0),\n",
    "                \"processing_time\": prog\n",
    "                     .get(\"durationMs\", {})\\\n",
    "                     .get(\"addBatch\", None)\n",
    "            })\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29fb7888-768e-4e07-9099-43f5a239dbd4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "6.b) PLOTTING RPOGRESS:"
    }
   },
   "outputs": [],
   "source": [
    "def plot_stream_metrics(progress_log):\n",
    "    \"\"\"\n",
    "    Given the output of monitor_streams() (a list of dicts),\n",
    "    clear the current figure and redraw both charts in-place.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "\n",
    "    # turn on interactive mode\n",
    "    plt.ion()\n",
    "\n",
    "    df = pd.DataFrame(progress_log)\n",
    "    if df.empty:\n",
    "        print(\"No data to plot yet.\")\n",
    "        return\n",
    "\n",
    "    # parse timestamps\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "\n",
    "\n",
    "\n",
    "    # two subplots: input_rows (top) and processing_time (bottom)\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "\n",
    "    for name, grp in df.groupby(\"query\"):\n",
    "        ax1.plot(grp[\"timestamp\"], grp[\"input_rows\"],    label=name)\n",
    "        ax2.plot(grp[\"timestamp\"], grp[\"processing_time\"], label=name)\n",
    "\n",
    "    ax1.set_ylabel(\"Input Rows\")\n",
    "    ax1.set_title(\"Stream: Input Rows Over Time\")\n",
    "    ax1.legend(); ax1.grid(True)\n",
    "\n",
    "    ax2.set_ylabel(\"Processing Time (ms)\")\n",
    "    ax2.set_title(\"Stream: Processing Time Over Time\")\n",
    "    ax2.legend(); ax2.grid(True)\n",
    "\n",
    "    # rotate & label the x-axis ticks\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.xlabel(\"Time\")\n",
    "\n",
    "    # force draw and pause so the UI updates\n",
    "    # clear_output(wait=True)\n",
    "    display(fig)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17df86c0-b696-4eb0-ad9b-de459d52fe92",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "8) MLFlow REGISTRATION"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def mlflow_run_calls(\n",
    "    y_true, y_pred,\n",
    "    model_name: str,\n",
    "    silver_delta_path: str,\n",
    "    run_name: str,\n",
    "    extra_params: dict = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Logs a single classification experiment to MLflow.\n",
    "    - y_true, y_pred: array-like ground truth & predictions\n",
    "    - model_name: name of your HF model\n",
    "    - silver_delta_path: path to your Silver Delta table\n",
    "    - run_name: human-readable name for this run\n",
    "    - extra_params: any other params (e.g. batch_size) to log\n",
    "    \"\"\"\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        # 1) Metrics\n",
    "        report = classification_report(y_true, y_pred, output_dict=True)\n",
    "        mlflow.log_metric(\"accuracy\", report[\"accuracy\"])\n",
    "        mlflow.log_metric(\"precision\", report[\"weighted avg\"][\"precision\"])\n",
    "        mlflow.log_metric(\"recall\",    report[\"weighted avg\"][\"recall\"])\n",
    "        mlflow.log_metric(\"f1_score\",  report[\"weighted avg\"][\"f1-score\"])\n",
    "\n",
    "        # 2) Confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        fig, ax = plt.subplots(figsize=(6, 5))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n",
    "        ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"Actual\")\n",
    "        ax.set_title(\"Confusion Matrix\")\n",
    "        plt.tight_layout()\n",
    "        cm_path = \"conf_matrix.png\"\n",
    "        fig.savefig(cm_path)\n",
    "        mlflow.log_artifact(cm_path)\n",
    "        plt.close(fig)\n",
    "\n",
    "        # 3) Params\n",
    "        mlflow.log_param(\"model_name\", model_name)\n",
    "        mlflow.log_param(\"mlflow_version\", mlflow.__version__)\n",
    "\n",
    "        if extra_params:\n",
    "            for k, v in extra_params.items():\n",
    "                mlflow.log_param(k, v)\n",
    "\n",
    "        # 4) Silver table version\n",
    "        silver_table = DeltaTable.forPath(spark, silver_delta_path)\n",
    "        version = silver_table.history(1).collect()[0][\"version\"]\n",
    "        mlflow.log_param(\"silver_table_version\", version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9365c118-3936-454e-a418-059441296289",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48acc819-f38b-4d83-8f49-8adb1e56943e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LOAD DATAFILES"
    }
   },
   "outputs": [],
   "source": [
    "def load_bronze_data():\n",
    "    return spark.read.format(\"delta\").load(BRONZE_DELTA)\n",
    "\n",
    "def load_silver_data():\n",
    "    return spark.read.format(\"delta\").load(SILVER_DELTA)\n",
    "\n",
    "def load_gold_data():\n",
    "    return spark.read.format(\"delta\").load(GOLD_DELTA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cfb7005-649a-4080-a22b-3d4bbe07193d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " **Bench Marking HF BatchSize:**\n",
    "  \n",
    "| Batch Size | Time (sec) | Tweets per Second |\n",
    "|------------|------------|-------------------|\n",
    "| 1          | 51.38      | 9.96              |\n",
    "| 8          | 16.68      | 30.70             |\n",
    "| 16         | 13.88      | 36.89             |\n",
    "| 32         | 16.06      | 31.88             |\n",
    "| 64         | 12.42      | 41.23             |\n",
    "| 128        | 12.16      | 42.11             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "875c5c5e-514a-4d73-ab9c-6c3b4a1bef46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  **Bench Marking HF BatchSize:**\n",
    "  \n",
    "# | Batch Size | Time (sec) | Tweets per Second |\n",
    "# |------------|------------|-------------------|\n",
    "# | 1          | 51.38      | 9.96              |\n",
    "# | 8          | 16.68      | 30.70             |\n",
    "# | 16         | 13.88      | 36.89             |\n",
    "# | 32         | 16.06      | 31.88             |\n",
    "# | 64         | 12.42      | 41.23             |\n",
    "# | 128        | 12.16      | 42.11             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a2f847b-01f0-41da-85be-bfb83e6a63c4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "BENCHMARK HF BATCH SIZE"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def test_bencmark_hf_model():\n",
    "    sample_texts = [\"I love this product!\"] * 512\n",
    "    batch_sizes = [1, 8, 16, 32, 64, 128]\n",
    "    results = []\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        pipe = pipeline(\"sentiment-analysis\", model=HF_MODEL_NAME, return_all_scores=True, batch_size=batch_size)\n",
    "        start_time = time.time()\n",
    "        pipe(sample_texts)\n",
    "        elapsed = time.time() - start_time\n",
    "        results.append({\n",
    "            \"batch_size\": batch_size,\n",
    "            \"time_sec\": elapsed,\n",
    "            \"tweets_per_sec\": len(sample_texts) / elapsed\n",
    "        })\n",
    "\n",
    "    benchmark_df = pd.DataFrame(results)\n",
    "    display(benchmark_df)\n",
    "    return benchmark_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15bea482-b19e-4c7c-b350-1402f3a3e50e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "IDLE WAIT and STOP"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def stream_until_idle(query, idle_rounds_required=3, interval_sec=10, verbose=True):\n",
    "    \"\"\"\n",
    "    Monitors a Spark StreamingQuery until it reports 0 input rows\n",
    "    for `idle_rounds_required` consecutive intervals.\n",
    "    \n",
    "    Parameters:\n",
    "        query:                StreamingQuery object (e.g., gold_query)\n",
    "        idle_rounds_required: How many idle rounds before stopping\n",
    "        interval_sec:         How often to poll\n",
    "        verbose:              Print status messages\n",
    "    \n",
    "    Returns:\n",
    "        List of progress snapshots (dicts)\n",
    "    \"\"\"\n",
    "    idle_rounds = 0\n",
    "    log = []\n",
    "\n",
    "    while query.isActive:\n",
    "        prog = query.lastProgress\n",
    "\n",
    "        if prog:\n",
    "            timestamp = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            num_rows = prog.get(\"numInputRows\", 0)\n",
    "            duration = prog.get(\"durationMs\", {}).get(\"addBatch\", None)\n",
    "\n",
    "            log_entry = {\n",
    "                \"timestamp\": timestamp,\n",
    "                \"input_rows\": num_rows,\n",
    "                \"processing_time_ms\": duration,\n",
    "                \"query\": query.name or \"<unnamed>\"\n",
    "            }\n",
    "            log.append(log_entry)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"🕒 {timestamp} | Rows: {num_rows} | Duration: {duration} ms\")\n",
    "\n",
    "            if num_rows == 0:\n",
    "                idle_rounds += 1\n",
    "                if verbose:\n",
    "                    print(f\"⚠️ Idle round {idle_rounds}/{idle_rounds_required}\")\n",
    "            else:\n",
    "                idle_rounds = 0  # reset idle counter on data\n",
    "\n",
    "            if idle_rounds >= idle_rounds_required:\n",
    "                if verbose:\n",
    "                    print(\"✅ Idle threshold met. Stopping stream...\")\n",
    "                query.stop()\n",
    "                break\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"⏳ Stream not yet reporting progress...\")\n",
    "\n",
    "        time.sleep(interval_sec)\n",
    "\n",
    "    return log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccb534a4-8799-44b6-8668-c5a9f9eb0bc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print('*********** UTILITES INSTALLED! GOOD TO GO!! *******')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "637ccd82-155f-4d81-b9e9-35df27a8254d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "utilities",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
